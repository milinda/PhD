# Introduction

Internet has become a central part of our daily lives and recent developments in
*Smart Phones*, *Internet of Things* and *Smart Homes* related technologies are going to make us more connected through Internet more than ever. As more and more people and devices are connected together via web applications which Internet is built, these application will produce massive amount of data at rates which we haven't experienced yet. In parallel to the increase of data generation, business entities and users require this data and information extracted or created out of this data available to them as fast as possible and trending towards decreasing latency for ever.

As we have experienced in recent past, traditional relational database technologies couldn't cope up with these ever increasing data rates, volumes and ever decreasing latency. So technology giants like Google, Yahoo and Amazon invented  novel storage technologies like GFS \cite{ghemawat2003google}, HDFS, Big Table \cite{chang2008bigtable} and Amazon Dynamo \cite{decandia2007dynamo} and distributed data processing frameworks such as Map-Reduce \cite{dean2010mapreduce}. As the trend continues, these systems which follow traditional pull based approach to answering queries couldn't handle the latency requirements and throughput requirements, *Distributed Stream Processing* platforms were built on top of modern messaging infrastructures to fulfill these requirements. While most of these new distributed stream processing frameworks are inspired by Map-Reduce and related big data technologies, most of the underlying concepts were from previous work.\textcolor{Red}{Add references and fix this.}

With this trend of moving to push based streaming processing platforms for reducing the latency and to handle increasing throughput requirements, several distributed and single-node stream processing platforms were implemented and they are continue to evolve to tackle ever changing requirements. But due to different architectural decisions underlying these platforms and variations in problem classes and non-functional requirements these frameworks are trying to address, these frameworks behave differently under different types of data and algorithms. \textcolor{Red}{Need references.} Also these systems support different types of programming abstractions. These variations make the decision of selecting a specific platform hard. Although there are existing results on how these platforms perform individually \textcolor{Red}{(Need references.)}, but its hard to find comparative studies. In this paper, we present a comprehensive framework for analyzing and comparing distributed stream processing platforms. Further, we implement benchmark algorithms from our framework on several popular stream processing platforms and present our findings evaluating their performance, scalability, fault tolerance and ease of use.

We envision that, having defined set of benchmark applications and a benchmarking process will help stream analysts and developers who requires to use these systems to make informed decisions. However functional and non-functional characteristics  of stream processing systems depends mainly on types of data, streaming analysis task, stream processing platform and the infrastructure used (persistence storage used to store output, messaging middleware used to deliver data streams). We try to minimalize the infrastructure differences across stream processing platforms  and propose a framework which can use to analyze stream processing platforms across three dimensions *type of streaming data, stream processing tasks,* and *stream processing platform*.

**Type of Streaming Data** - There are various types of data streams, some represent invidual events, some can represent graphs. If we consider a single event in a data stream as a tuple, there can be streams where tuples have hundreds of dimensions or in some cases less than 10 attributes. So our benchmark applications need to repsent these differences.

**Stream Processing Tasks** - According to \cite{streamdrill:presentation}, we can categorize stream processing tasks on two dimensions, complexity of task and latency requirements of task. For example counting, averages and count distinct can be very simple where as fraud detection, outlier detection or predictions based on streams of data can be complex. On the other hand reporting and monitoring applications may not have strong latency requirements. Even 1 minute delay would be fine in most cases. But applications which controls some machine or device based on streaming data may require sub millisecond or sub second latency requirements.

**Stream Processing Platform** - There are several different stream processing platforms available, both commercial and open source. \textcolor{Red}{Mention about several commercial, research and open source DSPs.} In this paper we are focusing only on Apache Storm, Apache Samza and Apache Spark Streaming. We believe that our approach is applicable to any other stream processing platform as well.

# Methodology

  Right     Left     Center     Default
-------     ------ ----------   -------
     12     12        12            12
    123     123       123          123
      1     1          1             1
