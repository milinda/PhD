The digitization of the world has fueled unprecedented growth in data, much of it driven by the global explosion of mobile data sources and the Internet of Things (IoT). 

Data from these connected devices is fueling a data economy, creating huge implications for future business opportunity. Additionally, the rate of growth of new data is creating a structural change in the ways enterprises, which are responsible for more than 80% of the world’s data, manage and interact with that data.

As the data economy evolves, an important distinction between the major ways in which businesses interact with data is emerging

data that has vol‐ ume and variety. Additionally, as companies embark on ever-more extensive big data initiatives, they have also realized the importance of interacting with data that is fast. The ability to process data imme‐ diately—a requirement driven by IoT macro-trends—creates new op‐ portunitu to realize value via disruptive busuiness models.

Data-driven applications are pervasive in many organizations and are characterized by utiliza‐ tion of data at scales previously unobtainable. This scale can refer to the complexity of the analysis, the sheer amount of data being man‐ aged, or the velocity at which data must be acted upon.

In the new paradigm—one in which data in motion has equal or greater value than “historical” data (data at rest) —new opportunities to extract value require that enterprises adopt new approaches to data management.

"in-memory database architectures mean familiar tools can be used to tackle fast data."

The enterprise data architecture can be rep‐ resented as a data pipeline that unifies applications, analytics, and ap‐ plication interaction across multiple functions, products, and disci‐ plines 

Most technologists understand that data exists on a time continuum; it is not stationary. In almost every business, data moves from function to function to inform business decisions at all levels of the organiza‐ tion. 

Immediately after data is created, it is highly interactive and for each event, of greatest value. This is where the opportunity exists to perform high-velocity operations on “new” or “incoming” data

With the adoption of fast and big data technologies, a trend is emerging in the way data management applications are being architected, de‐ signed, and developed.

This natural progression quickly takes people to the point at which they realize they need a unifying architecture to serve as the basis for how data-heavy applications will be built across the company, encompassing application interaction all the way through to exploratory analytics.

Interacting with fast data is a fundamentally different process than interacting with big data that is at rest, requiring systems that are ar‐ chitected differently. 


The big data portion of the architecture is centered around a data lake, the storage location in which the enterprise dumps all of its data.

SQL- on-Hadoop systems exist for a couple of important reasons:
a. SQL is still the best way to query data
b. Processing can occur without moving big chunks of data around


The predominant way in which this integration occurs today, and will continue for the foreseeable future, is through an extract, transform, and load (ETL) process that extracts, transforms as required, and loads legacy data into the data lake where everything is stored. 

critical requirements, which include the ability to ingest and interact with the data feed(s), make decisions on each event in the feed(s), and apply realtime analytics to provide visibility into fast streams of incoming data.

But ingesting this data is the smallest and simplest of the tasks required in managing these types of streams. 

This is the concept of “data fusion,” the ability to make contex‐ tual decisions on streaming and static 
data.

the architecture of the fast data portion is equally critical to the data pipeline.

all serious data growth in the future will come from fast data. Fast data comes into data systems in streams; they are fire hoses. These streams look like observations, log records, interactions, sensor readings, clicks, game play, and so forth: things happening hundreds to millions of times a second.

most valuable improvements come when these interactions are spe‐ cific to the context of each event (i.e., use the current state of the sys‐ tem) and occur in real time.

ingest events, interact with data for decisions, and use realtime analytics to enhance the experi‐ ence. 

But application developers now realize applications must interact in real time with fast streams of data and use the analytics derived to make valuable interactions. The stresses this places on the fast data architecture necessitates new approaches that can meet these needs.

Further pushing the adoption of fast data within the enterprise data architecture are the widespread—and divergent—expectations about what can be achieved with analytics.


This brings us to the fast data segment of the enterprise data architecture, where we deal with fast data to make bet‐ ter, faster realtime applications

Systems designed to handle fast data must merge the capabilities of three historical products: operational databases, realtime analytics, and stream processing.

 Fast data is a new frontier. It is an inevitable step or‐ ganizations will take when they begin to deeply integrate analytics into the data management architecture.

 Much of the interesting data coming into organizations today is com‐ ing fast, from more sources, and at a greater frequency. 

 Moreover, each reading needs to be examined to determine the status of the sensor and whether interaction is required.

 Using other pieces of data—previous events, events from other end‐ points, static data—to make decisions on how to respond enhances the interaction described previously; it provides much-needed context to decisions.

 If an event is taken only at its face value, the architecture is missing the context—the current state—in which the event occurred. The ability to make better decisions because of things you know about the entire application is lost.


 The knowledge gained by pro‐ cessing big data should inform decisions. Moving that knowledge to the front of the data pipeline using an in-memory operational database enables decisions, driven by deep analytical understanding, to be op‐ erationalized and acted on each time an event enters the system.

 

