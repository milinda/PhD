

\documentclass[10pt, conference, compsocconf]{IEEEtran}

\usepackage{color,soul}
\usepackage{listings}
\usepackage{hyperref}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{frame=lines}

\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi



% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{SamzaSQL: Open Source Distributed Fast Data Management System}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

% \author{\IEEEauthorblockN{Milinda Pathirage\IEEEauthorrefmark{1} and
% Beth Plale\IEEEauthorrefmark{2}}
% \IEEEauthorblockA{School of Informatics and Computing,
% Indiana University\\
% Email: \IEEEauthorrefmark{1}mpathira@indiana.edu,
% \IEEEauthorrefmark{2}plale@cs.indiana.edu}
% \and
% \IEEEauthorblockN{Yi Pan\IEEEauthorrefmark{3} and
% Chris Riccomini \IEEEauthorrefmark{4}}
% \IEEEauthorblockA{LinkedIn\\
% Email: \IEEEauthorrefmark{3}yipan@linkedin.com,
% \IEEEauthorrefmark{4}criccomini@linkedin.com}
% \and
% \IEEEauthorblockN{Julian Hyde\IEEEauthorrefmark{5}}
% \IEEEauthorblockA{Hortonworks\\
% Email: \IEEEauthorrefmark{5}julian@hydromatic.net}
% }

\author{
    \IEEEauthorblockN{Milinda Pathirage\IEEEauthorrefmark{1}, Yi Pan\IEEEauthorrefmark{2}, Chris Riccomini\IEEEauthorrefmark{2}, Julian Hyde\IEEEauthorrefmark{3} and Beth Plale\IEEEauthorrefmark{1}}
    \IEEEauthorblockA{\IEEEauthorrefmark{1}School of Informatics and Computing, Indiana University
    \\\{mpathira, plale\}@indiana.edu}
    \IEEEauthorblockA{\IEEEauthorrefmark{2}LinkedIn
    \\\{yipan, criccomini\}@linkedin.com}
    \IEEEauthorblockA{\IEEEauthorrefmark{2}Hortonworks
    \\julian@hydromatic.net}
}

% make the title area
\maketitle


\begin{abstract}
In a data-driven economy, it's important act on data as it arrives to stay competitive. Specialized message queues like Kafka and specialized streaming processing platforms like Apache Samza was invented to take value out of fast data. But these system still lacks support for standard querying capabilities which is available in \textit{Big Data} processing systems like \textit{Hive} or \textit{Presto}. In this paper, we describe a SQL based streaming data querying and manipulation framework implemented on top of Apache Samza -- a open source stream processing framework.
\end{abstract}

\begin{IEEEkeywords}
Big Data; SQL; Streaming;

\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
In today's data-driven economy, organisations -- whether \textit{big} or \textit{small} -- depends on data analytics to stay competitive. Advancements in \textit{Big Data} related technologies such as Hadoop have transformed how organisations interact with data by enabling analysis of petabytes of data using commodity clusters. Often these massive quantities of data is created by events occuring thousands to tens of thousands of times per second. We these data generation rates we not only talks about volume of data, we also talks about velocity of data. To get the most out of this data we have to act on this data as it arrives. Couple of years ago, it was impracticle to act on data as it arrives due to higher latencies of Big Data processing tools and cost of specialized software and hardware. 

As a result,  message queues and stream processing systems like \textit{Kafka}, \textit{Apache S4}, \textit{Apache Storm} and \textit{Apache Samza} which are robust, fault tolerant and scalable were invented to take most out of the fast moving data. These systems allows us to act on data as it arrives, generating value to both organisations and users which wasn't possible half a decade ago at this scale.

While it is important to act on data as it arrives, it is also important for these fast data processing pipelines to interact with static data or output data from batch jobs for the purpose of data enrichment or near real-time analysis. Reference architecture known as Lambda Architecture was proposed as a solution to this hybrid analytics requirements and there are implementations of this architecture such as Summingbird \cite{boykin2014summingbird} and RADStack \cite{metamarkets:radstack} in use today. Instead of widely used declarative language like SQL, these frameworks including Summingbird provide low level APIs in languages like Java or custom query languages specific for the framework. But custom programs written for specific purposes add maitenance overhead and often hard to reuse. And custom query languages doesn't have the adavantages of extensions to standard SQL such as 

\begin{itemize}
\item easy to learn if you know SQL
\item clear semantics 
\item allows combining stored relaitons and streams in a single query
\end{itemize}

In this paper, we present \textit{SamzaSQL}, an open-source SQL based  streaming query engine implementation with support for interacting with non-streaming data sources on top of Apache Samza. SamzaSQL supports queries expressed in standard SQL with minor extensions to add stream producing and windowing capabilities. These queries get compiled into streaming jobs executed in Samza. Depending on the query, these streaming jobs may involve access to static data sources such as stored relations and may access outputs from recurring batch jobs. SamzaSQL front-end, which does query parsing, validation and planning is built on top of \textit{Apache Calcite} and supports querying stream data formats such as Avro or JSON. SamzaSQL also includes support for pluggable catalogs or schema registries based on Calcite's schema factory functionality. This allows us to extend SamzaSQL to support various data sources with custom formats. SamzaSQL's streaming query execution layer is implemented based on CQL \cite{arasu2006cql} continuous query execution model.

Until now, we have used custom programs or data processing pipelines built on APIs provided by streaming and batch data processing frameworks or programming models such as Summingbird \cite{boykin2014summingbird} to build Lambda or Kappa architecture style applications. General motivation of this work is to explore how we can provide a unified framework -- which enables both Lambda Architecture and Kappa architecture style data processing pipelines based on well known SQL related standards and continuous query execution model presented in CQL \cite{arasu2006cql}. \hl{TODO: Revisit this because window operator design proposed by Yi may need us to make changes to query execution model.}

Windowing is a core and central concept of stream querying due to the fact that operators like \textit{join} and some of the aggregations cannot be evaluated over unbounded streams. SamzaSQL comes with a widow operator implementation with supports for 

\begin{itemize}
\item timely and deterministic window output under unpredictable message delevery latencies
\item and, deterministic window output with node failures and message re-delivery
\end{itemize}

\hl{TODO: We are mainly targeting soft and near real-time systems. We need to describe this. Also we need to discuss about partition based parallelism supported.}

\section{Motivation}
In todays data driven economy, more and more business organisations depend on timely analytics of data they accumulate to stay competitive. There is a saying that before becoming big the data was fast. This is because of the fact that most of the big data are result of accumulating events from sensors, devices, click streams, server logs and other form of user activity logs. Normally big data is generated by these fast data or events happening at scale of tens of thousands to hundreds of thousand events pr second. To get most out of this data, timely analytics is needed. We can categorize these data processing systems or tasks into three categories based on their latency requirements.

\begin{itemize}
  \item Online/Realtime - Queries on fast data in realtime and may refer slow-changing or static data for data enrichment. Most of the times latency requirements are in the order of milliseconds
  \item Nearline - Queries on fast data but latency requirements are in the order of seconds. \textit{Sooner these systems provide results, the higher the value to the end user \textbf{from lquid}}.
  \item Offline - Latency requirements are in order of minutes or hours. OLAP queries and Map/Reduce jobs are typical examples.
\end{itemize}

Above mention analytics tasks are often interconnected \textbf{from trill} and often realtime and nearline analytics tasks used historical information which may be results from offline/batch analytics tasks. Different solutions and architectures were proposed to unify the way we handle this interconnection between different types of analytics. 

Lambda arhcitecture is one of these architectural frameworks and there several implementations available. In this architecture, there are three main components or layers to handle fast data, big data and serving queries based on results from fast data component and big data component. In the context of Lambda architecture these layers are know as

\begin{itemize}
 \item Speed Layer
 \item Batch Layer
 \item Service Layer
\end{itemize}

Then there is another group of people who propsed completely different architecture know as Kappa architecture for handling this problem. Their argument is to use stream processing system with materialized views to handle both speed and batch analytics tasks. You will expressed your analytics task as a stream processing algorithm and replying input streams will be used to simulate the batch layer. One advantage of this method is you can reuse the same applicaiton/algorithm for both speed and batch layer with extra management capabilities in stream processing framework to support replay and materialize view management.

But as we know, current implementations of Lamda architecture and Kappa architecture both require users to use a programming APIs in high-level languages or query languages that looks like SQL (Druid) but with different set of semantics. Also our experience with Lambda architecture framework like Summingbird shows that understanding of complex programming abstractions are required to take the advantage of these frameworks. Also managing data analytics applications written for these may incur extra maintenance overhead. 

So Kappa/Lambda arhcitecture implementation which provides DBMS like capabilities is required to make these fast data analytics frameworks accessible to broad community of users. Wide adoption of Hive and Drill like frameworks to query Big Data is a good indicator for requirement of SQL based analytics for Big Data and in this work we try to provide same capabilities provided by Hive for Big Data to Fast Data analytics using Samza SQL. Also we envision that SQL compliant fast data management system will make fast data analytics accessible to broad community of users because they can utilize knowledge of SQL in the context of fast data analytics. 

Also its a known fact that online and nearline analytics often interact with static or slow-changing data sources. So we wanted our system to allow access to slow-changing data sources within streaming SQL queries. In addition to that to enable users to utilize their existing knowledge of SQL, its important to support standard SQL with minimal extensions to support streaming specific scenarios. 

Given that today's applications use different data formats such as JSON, Avro or Protocol Buffers to communicate, its important that the fast data management solution supports pluggable message formats. 

Also fault tolrenace and scalability is impotant at the internet scale and its important that this SQL based solution take adavantage of existing fault tolrenance and scalability features in the underlying stream processing system. 

When processing streaming data, its important to have support for proper window operations to handle different windowed aggregation scenarios and these window processing infrastrcture should support fault tolerance and delayed events.

Even though Kappa architecture sounds promising, but adding a SQL layer in-front solves several drawbacks of Lambda architecture such as having to maintain two different code bases for streaming and batch. So its important this framework to support compiling of SQL queries to both batch and streaming tasks.

\section{System Overview with Example}
Consider a stream of ad click events, where events are encoded in Avro format with the schema:

\begin{lstlisting}
{
     "type": "record",
     "namespace": "org.apache.samza",
     "name": "AdClick",
     "fields": [
       { "name": "clickTime", "type": "long" },
       { "name": "userId", "type": "long"},
       { "name": "adId", "type": "long"}
     ]
}
\end{lstlisting}

User wants to compute, for each ad, a 5-minute windowed count of clicks on that ad, across a 5% sample of users in near realtime.

\subsection{User Experience}
Its theoritically possible to using any type of messaging system to input data streams to Samza. But Kafka is the preffered and stable messaging infrastructure for Samza. Lets assume 

\begin{itemize}
  \item events are published to a Kafka topic \textit{adclick}, partitioned on adId
  \item \textit{adclick} topic is registered in Kafka \textit{Schema Registry} known to Samza SQL query driver
\end{itemize}

When connected to a Kafka Schema Registry instance, Samza SQL driver automatically lazy loads stream definitions into the query planner's in-memory metadata store and topic registered in that instance of Schema Registry instance will be visible to the query planner with correct type informations. So the user can write the above mentioned  query logic as a Samza SQL query:

\begin{lstlisting}
  clickTime | userId    |  addId 
------------+-----------+---------
 10:00:00   |        30 |       2 
 10:02:43   |        10 |       1  
 10:04:23   |        20 |       1  
 10:06:10   |        10 |       3  
 10:04:56   |        40 |       2 

SELECT STREAM 
  round5min(clickTime to timestamp) as clickTime,
  COUNT(*) as clicks
FROM adclick
WHERE userId % 100 < 5
GROUP BY round5min(clickTime to timestamp), adId;

  clickTime | userId    |  addId 
------------+-----------+---------
 10:00:00   |        30 |       2 
 10:00:00   |        10 |       1  
 10:00:00   |        20 |       1  
 10:05:00   |        10 |       3  
 10:00:00   |        40 |       2
 \end{lstlisting}

 The result of the above query will be a stream. At every 5-minutes Samza SQL will emit click count of ads where there was ad clicks in previous 5-minutes. Late arrivals will be handled according to the windowing policy and updated count will be emitted to the output stream. Samza SQL infer that it needs to emit ad click counts at every 5-minutes based on the fact that it knows *clickTime* is increasing  and also knows that round5min(clickTime to timestamp) is also increasing. For example once Samza SQL sees a row at or after 10:05:00 it knows that, it needs to emit a patial ad click count for last 5 minutes and when there is a another ad click event belongs to 10:00:00 to 10:05:00 interval it will update previous count and re-emit the counts for that time period until window close timeout is passed. 

The columns or expressions that is increasing or decreasing is know as *monotonic* column/expression. Without a monotonic expression in GROUP By query, Samza SQL can't make any progress in streaming queries and will throw a error at query validation phase. 

To handles aggregates like above, Samza SQL implements a relaxed version of SQL stream aggregation where rows can be out of order to accomodate late arrivals common in distributed data streams. Samza SQL window aggregates emit partial results as soon as possible and handle the out of order tuples according to windoing policy. For Samza SQL tumbling window aggregate like above, single input row can be contributed to multiple output rows due to partial aggregation which is completely different from standard SQL aggregate semantic where each input row contributes to only one output row.

If user wants this output stream to be appear in a another stream he can use INSERT INTO query to deliver the results of this query to an another stream.

\begin{lstlisting}
INSERT INTO adclickcounts
  SELECT STREAM 
    round5min(clickTime to timestamp) as clickTime,
    COUNT(*) as clicks
  FROM adclick
  WHERE userId % 100 < 5
  GROUP BY round5min(clickTime to timestamp), adId;
\end{lstlisting}

Some other important use cases in a streaming query context are joining with stored relations and windowed joins. Samza SQL provide support for those constructs as well, but the full description on query implementaiton is outside the scope of this paper. But we note that Samza SQL supports all the basic standard SQL constructs on both streams and relations.

Samza SQL is quite different from existing systems which implements Lambda architecture and Kappa architecture mainly because Samza SQL is build based on relational algebra and it views streams as relaitons that varies with time similar to CQL. This enabled easy integration of slow-changing data into streaming queries as well as provides consistent view across both streaming and relational data sources.

\subsection{System Overview}

Samza SQL first compile user query into a logical query plan and then optimize this logical query plan based on different heuristics and metrics related streaming. Then this logical query plan get converted into a physical plan consisting Samza SQL operator DAG. Each of these operators consume one or more streams of tuples and produces a stream of tuples where tuple represents a single stream element. Samza pulls tuples from input streams (sources) and push them to the operators. Elements in a stream can be formatted using a message format like Avro or JSON and Samza SQL provide plugging architecture to integrate new message formats to the framework. 

Samza SQL supports queries on both streams and tables where

\begin{itemize}
  \item A stream is a (possibly infinite) sequence of elements partitioned by user selected key or partitioning mechanism and a partition is an ordered, immutable sequence of elements.
  \item A table is analogous to tables in relational databases, but not limited only to relational databases. Samza SQL provides extension mechanisms necessary to support different types of slow-changing data sources based on user preference.
\end{itemize}

The output from streaming SQL queries can be ingested into an another stream or a database for further consumption. Query results can laso be materialized into materialized views. Samza SQL encourage ingesting streaming outputs into an another stream for easily handling of partial aggregates caused by out of order arrival of data. Most important requirements and Samza SQL design decisions which caters those requirements are summerized below and discussed in the coming sections including a performance evaluation of current implementation.

\subsubsection{Familiar language}

Samza SQL uses extended version of standard SQL which makes it easy to learn for anyone who knows SQL. As a result this results in clear semantics where we try to produce same results on a stream as if the data were in a table. For example, Samza SQL produce same result for tumbling window query 1 if there aren't any out of order tuples. But in case of out of order tuples there can be multiple partial emits until window is closed. Also using extended version of standard SQL helped us to utilize existing query parsing and optimization framework in the front-end. This reduces development time as well as it gave us lot of oppertunity focus on the streaming aspects of the system.

\subsubsection{Early results and incremental processing}

Its important to support incremental processing in streaming query engines. Samza SQL implements all the aggregators in incremental fashion and different window policies are employed to produce partial early results in the precense of out of order arrivals. Samza SQL detect queries of blocking nature early at the query validation phase to prevent accidental infinitely blocking queries during runtime.

\subsubsection{Support both streams and relations}

Samza SQL supports both streams and relations from slow-changing data sources like NoSQL storages and relational databases. We are utilizing Calcite capability to generate query plans consisting of scanning both streams and relations to implement this functionality for Samza SQL. This 

\subsubsection{Parallesim and Scalability}

Initial implementation of Samza SQL supports data parallelism using Kafka's partitioning capabilities. Each partition will be handled by Samza SQL task specific to that partition. Task parallelism for scaling large queries are not supported yet.

\subsubsection{Fault tolerance}

Samza SQL achive fault tolerance using Samza's local storage and reply capability. Local state of Samza SQL tasks will be backed up as a change stream and incase of a failure tasks will get restored with local state containing data upto last know checkpoint. In addition to that Samza SQL will checkpoint window processing into a another change stream such that it can replay tuples lost due to task failure from Kafka after restoring the local state.

\subsubsection{Ability to replay}


\subsubsection{Materialized views}



\section{SamzaSQL}
SamzaSQL is a streaming query layer on top of Apache Samza stream processing framework and utilizes Apache Calcite \cite{asf:2014:calcite} for query planning. 

\subsection{Data Model}

\begin{itemize}
\item Streams - A stream is a (possibly infinite) sequence of elements. By default, each stream in Samza has a corresponding Kafka \cite{kreps2011kafka} topic. There can be multiple partitions for each topic in a Kafka cluster. A element of a stream/topic can be in any format (Generally, elements in the same topic is in same data format). Users can associate stream with the serialization format of the underlying data. SamzaQL provides support for several serialization formats such as Avro, JSON and users can also add new serialization formats by implementing \textit{SerdeFactory} and \textit{Data} interfaces. In the current version, Kafka Schema Registry based Calcite Schema implementation provides stream meta-data necessary to query compilation and planning.
\item Partitions - Each partition is an ordered, immutable sequence of messages that is continually appended to a commit log. The elements in the partitions are each assigned a sequential id number called the offset that uniquely identifies each element within the partition. SamzaQL guarantees the ordering of elements within the partition, but not inbetween partitions.
\item Tables - Analogous to tables in relational databases, but not limited only to relational databases. \textit{Tables} in SamzaQL can refer to any static data source such as \textit{HBase} table, \textit{Espresso} table  which is exposed to SamzaQL environment as relational table. \textit{TODO: Describe semantics little bit more. May need to consider partitions of tables too.}
\end{itemize}

SamzaQL supports primitive column types (integers, floating
point numbers, generic strings, dates and booleans) and
nestable collection types - array, map and object. Users can also define their own types via Calcite's JavaType which allows use of a Java Class as a type.

\subsection{Query Language}
SamzaQL supports standard SQL with extensions for stream queries. Data definition (DDL) statements are not supported at this stage and SamzaQL depends on Kafka schema registry and Calcite schema models to provide metadata about streams and tables to the query compiling and planning phase. Custom serialization formats are supported through Calcite's schema models.
Streams in SamzaQL is immutable and append only, so update and delete operations are not supported on streams. SamzaQL also supports user defined functions for data transformation and aggregations via a Java API.

Streaming specific extensions were added to standard SQL supported by Calcite to support \textit{relation-to-stream} transformation, flexible window specification and stream partitioning.

\begin{itemize}
\item \textbf{Relation-to-stream transformation}: Following CQL \cite{arasu2006cql} continuous query execution model, SamzaQL first transform the stream into a \textit{time-varying relation} based on the windows specification in the query or using defaults. And query body is executed on this/these time-varying relation/s. So to convert the time-varying relations results from query execution to streams, we use three basic operations  \textit{ISTREAM}, \textit{DSTREAM} and \textit{RSTREAM} from CQL. To support this, we have extended SQL \textit{SELECT} statement with \textit{STREAM},\\ \textit{DSTREAM} and \textit{RSTREAM} keywords which should come right after \textit{SELECT} keyword like below.

\begin{lstlisting}[language=SQL]
SELECT [STREAM | DSTREAM | RSTREAM] ... FROM ...
\end{lstlisting}

\item \textbf{Window Specification}: This is sitll under discussion. But there are some special cases we need to support except well known \textit{tumbling}, \textit{sliding} and \textit{hopping} windows. e.g. Consider that we want to have a count of stock trades (as a infinite stream) happened in the last hour, but only every 11min. It is easy to write the first part in sqlstream as:

\begin{lstlisting}[language=SQL]
SELECT STREAM rowtime, count(*) 
              OVER (ORDER BY rowtime RANGE INTERVAL '1' HOUR PROCEDING)
  FROM Trades
\end{lstlisting}
The above will create a stream of counts that happened every hour continuously as rows are scanned

Now here is the question:
\begin{itemize}
\item How do we have the count every 11min instead of as the row comes in?
\item If there is no row in Trades between 12:00pm to 2:00pm, how do we tell the system to still generate 0 counts for the time moments: 12:11pm, 12:22pm, 12:33pm, etc.? Or, those rows are delayed in the delivery in the system and user wants to ignore late-arrival of messages after 5min timeout to close the counting window? How can we support that use case w/o breaking SQL grammar?
\end{itemize}

\item \textbf{Stream Partitioning}: Not yet decided on how to integrate partitioning. But requirements are as follows:
\begin{itemize}
\item On the producer-side (when inserting query results to a stream), we have no way to determine which key to use for partitioning and how many partitions an output stream should be right now. So there should be a way to specify this in query.
\end{itemize}

\end{itemize}

In addition to this, there are situations where queries which are valid in the context of relation databases are no longer valid in streaming scenarios due to non-blocking nature of streaming queries. So, we need to extend Calcite with custom validation queries. These things are still under discussion.

\subsection{Windowing}
\subsection{Aggregates}
\subsection{Joins}




\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

% that's all folks
\end{document}


